{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to your second assignment!\n",
    "\n",
    "## Introduction/Motivation\n",
    "\n",
    "LIGO is a Gravitationa Wave detector that sits still waiting for GW travelling our cosmos to stretch and squeeze its arms to jump and say we **might** have a signal. LIGO has 2 detectors, one in Hanford, Washington and the other in Livingstone, Louisiana. It is in-principle just a fancy Michelson Inteferometer with a Fabry-Perot cavity. It estimates the strain in its arms (proxy for spacetime fabric) using the interference pattern you obtain in the detector. The dataset one gets from LIGO is of the form (gps_time, strain).\n",
    "\n",
    "**Note** - Mind you LIGO has plenty of auxiliary channels that look at various systems of the detector that can be used to predict sources of systematic error, understand what changes in detector cause what changes in your data, etc.\n",
    "\n",
    "\n",
    "In the field of Gravitational Waves, data analysis is majorly dominated by a very naive method. We simulate a waveform based on a unique set of parameters - mass of $1^{st}$ body, mass of $2^{nd}$ body, distance, location in the sky - and try to match the detected signal to our simulated waveform to see how good a match we get. Currently LIGO detects GW from sources with a combined mass in the range 10-140 solar masses. As you can see, the parameter space for simulating such waveforms is enormous. It is impractical to perform this task everytime you come across something that might be a signal. \n",
    "\n",
    "ML techniques are on the up and coming in this field. We will be working on a made-up problem where we will be estimating the distance to a GW source given a set of parameters. \n",
    "\n",
    "\n",
    "## The Problem\n",
    "\n",
    "You will be provided with a csv file 40k rows of data of the form **(m1, m2, inc, ra, dec, SNR, dist)**. All the data points are simulated using the following ranges \n",
    "1. m1, m2 = [8, 100]\n",
    "2. ra + dec = uniform_sky i.e., uniformly spread across the sky\n",
    "3. distance = [500, 700] Mpc\n",
    "\n",
    "You are to use the first 6 parameters to predict the last parameter. The goal of this assignment is to be able to predict the distance of the GW source given the SNR and the parameters of the source. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "Import relevant packages. Make sure to import the <a href=\"https://docs.python.org/3/library/csv.html\" target=\"_blank\">csv</a> module for reading data and modules required to make ROC curves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# One PDF for all graphs\n",
    "outputname = 'GW_classification_week4.pdf'\n",
    "pp = PdfPages(outputname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Read the datafile and check if the dimensions make sense.\n",
    "\n",
    "Data files - \n",
    "1. GW_40k.csv\n",
    "2. GW_30k_BinaryTesting.csv\n",
    "\n",
    "The columns of the file are separated by commas and not tabs/space so make sure to set appropriate delimiters.\n",
    "\n",
    "Tip:\n",
    "\n",
    "1. If you are familiar with <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.loadtxt.html\" target=\"_blank\">numpy.loadtxt</a>, then you can use that instead of csv. Make sure to remember which index corresponds to which column\n",
    "\n",
    "2. If you prefer using pandas as in the previous assignment then feel free to use <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\" target=\"_blank\">pd.read_csv()</a>.\n",
    "\n",
    "3. If you want to try out the csv module then refer <a href=\"https://docs.python.org/3/library/csv.html\" target=\"_blank\">csv</a> and take a look at <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.ndarray.astype.html\" target=\"_blank\">numpy.ndarray.astype()</a>\n",
    "\n",
    "### We request you to not change any variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip: If you wish to comment multiple lines at once try enclosing the lines like ''' ... '''\n",
    "\n",
    "# If you are using pandas.read_csv() then comment the other lines of code\n",
    "file = \"\"      # Filename as string\n",
    "cols = [0, 1, 2, 3, 4, 5, 6] # Columns to read\n",
    "col_names = ['m1', 'm2', 'inc', 'ra', 'dec', 'SNR', 'dist']  # Column names\n",
    "\n",
    "# Read using pandas.read_csv().\n",
    "data_set = \n",
    "\n",
    "\n",
    "# If you are using numpy.loadtxt() then comment the other lines of code\n",
    "file = \"\"  # Filename\n",
    "\n",
    "# Read using numpy.loadtxt()\n",
    "data_set = \n",
    "\n",
    "\n",
    "# If you are using csv.reader() comment the other lines of code\n",
    "file = \"\"     # Filename\n",
    "data_set = []           # Empty array to append datapoints to\n",
    "\n",
    "# Each row is a datapoint. Read rows one-by-one and append the empty array\n",
    "with open(file, newline='') as csv_file:\n",
    "    # Define a reader object\n",
    "    csv_reader = \n",
    "    for row in csv_reader:\n",
    "        # Add data points to the empty array\n",
    "\n",
    "data_set =   # Array to numpy array. Note that this array contains only strings.\n",
    "data_set =     # Convert string array into float\n",
    "\n",
    "print(\"Shape of our dataset (No-of-points, dimensions): {}\".format(data_set.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Construct your Y_train and Y_val arrays and strip the X_train and X_val of its distance column. \n",
    "\n",
    "Rules for constructing Y_train and Y_test \n",
    "1. dist $\\leq$ 600 $Y_i$ = 0\n",
    "2. dist $>$ 600 $Y_i$ = 1\n",
    "\n",
    "\n",
    "Tip:\n",
    "1. If you are using pandas method then this might come in handy - <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html\" target=\"_blank\">pd.DataFrame.drop</a>. When using pandas method make sure to pass \"inplace\" argument if needed. **Another clue - axis = 1**\n",
    "2. If you are using the numpy method or csv method make sure to keep an eye out for the cols[] array from the previous block to know what each column represents. Take a look at <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.delete.html\" target=\"_blank\">numpy.delete()</a> if you are finding it hard to delete columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you used pandas to prepare your arrays in the previous step then comment out all the other lines of code\n",
    "\n",
    "# Add a new column which contains 0s and 1s according to the rule mentioned above\n",
    "data_set['y_value'] = \n",
    "\n",
    "# Store the 'y_value' column in a new array. This is so that we can split the array into train-val-test easily\n",
    "Y = data_set['y_value']\n",
    "\n",
    "# Delete 'dist' and 'y_value' columns\n",
    "\n",
    "\n",
    "\n",
    "# If you used numpy.loadtxt() or csv method to read your data comment the other lines\n",
    "\n",
    "# Define an empty-array/ 0-array that will correspond to each data-point\n",
    "Y = \n",
    "for i in range(data_set.shape[0]):\n",
    "    # Add Y values according to the rules mentioned above\n",
    "\n",
    "# Delete 'dist' column\n",
    "data_set = \n",
    "\n",
    "\n",
    "# Spilt your data set into training set and test set.\n",
    "# You might have to use the function from last assignment twice\n",
    "x_train, x_test, y_train, y_test = \n",
    "x_val, x_test, y_val, y_test = \n",
    "\n",
    "# To get an idea of what number of datapoints we are working with\n",
    "print(\"Training array - X: {} Y: {}\".format(x_train.shape, y_train.shape))\n",
    "print(\"Validation array - X: {} Y: {}\".format(x_val.shape, y_val.shape))\n",
    "print(\"Test array - X: {} Y: {}\".format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 4\n",
    "\n",
    "Create your model\n",
    "\n",
    "Links - \n",
    "1. <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\" target=\"_blank\">Sequential()</a>\n",
    "2. <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\" target=\"_blank\">Dense()</a>\n",
    "3. <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\" target=\"_blank\">BatchNormalization()</a>\n",
    "4. <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile\" target=\"_blank\">compile()</a>\n",
    "5. <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/Model#summary\" target=\"_blank\">summary()</a>\n",
    "\n",
    "Tips - \n",
    "1. Use axis = 1 for BatchNorm if you use BatchNorm at all\n",
    "2. Use **adam** as your optimizer when calling compile()\n",
    "3. use **binary_crossentropy** for loss\n",
    "4. use **accuracy** for metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the number of features\n",
    "n_features = \n",
    "model = tf.keras.models.Sequential([\n",
    "    # Add your layers\n",
    "    ])\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "# Try to get a summary. You might not get one until you train your network. \n",
    "# If this throws an error then delete this line of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "Train your model using <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\" target=\"_blank\"> model.fit()</a> \n",
    "\n",
    "Tips - \n",
    "1. Make sure to use batches since we are dealing wtih huge numbers of data points.\n",
    "2. Make sure to use **steps_per_epoch** argument and **batches** to get a smoother descent\n",
    "3. Make sure your **batch_size * steps_per_epoch equals** the size of your train_data_set\n",
    "4. Make sure your **batch_size * validation_steps equals** the size of your validation_data_set\n",
    "5. Make use of <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\" target=\"_blank\">EarlyStopping()</a> if are not sure about the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    # parameters\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6\n",
    "\n",
    "Plot history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can choose to simply leave this block as is and you will get results\n",
    "# If you really want to plot loss and accuracy yourself feel free to comment the following lines\n",
    "# Please keep the plt.savefig() lines undisturbed\n",
    "\n",
    "# Create a pandas Dataframe which would contain various columns (i.e., val_loss, loss, val_acc, acc etc) \n",
    "# and values or those columns correspoding to each epoch \n",
    "df_loss_acc = pd.DataFrame(history.history)\n",
    "\n",
    "# Select the training and validation losses\n",
    "df_loss= df_loss_acc[['loss','val_loss']]\n",
    "\n",
    "# Renaming for clarity\n",
    "df_loss.rename(columns={'loss':'train','val_loss':'validation'},inplace=True)\n",
    "\n",
    "# Selecting accruacies and renaming for clarity\n",
    "df_acc= df_loss_acc[['accuracy','val_accuracy']]\n",
    "df_acc.rename(columns={'accuracy':'train','val_accuracy':'validation'},inplace=True)\n",
    "\n",
    "# Plotting and saving train_loss & val_loss vs epoch\n",
    "df_loss.plot(title='Model loss',figsize=(12,8)).set(xlabel='Epoch',ylabel='Loss')\n",
    "plt.savefig(pp, format='pdf')\n",
    "\n",
    "# Plotting and saving train_acc & val_acc vs epoch\n",
    "df_acc.plot(title='Model Accuracy',figsize=(12,8)).set(xlabel='Epoch',ylabel='Accuracy')\n",
    "plt.savefig(pp, format='pdf')\n",
    "\n",
    "# You can write your own code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7\n",
    "\n",
    "Test your model and plot ROCs. \n",
    "\n",
    "You can pretty much copy paste the code from your last assignment here and it should work. Although you might have to change a few variable names here and there.\n",
    "\n",
    "Links - \n",
    "1. <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate\" target=\"_blank\">model.evaluate()</a>\n",
    "2. <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict\" target=\"_blank\">model.predict()</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your model using the test_data_set\n",
    "test_loss, test_acc = \n",
    "print(\"Test_set performance\\nTest_Loss = {}\\tTest_accuracy = {}\".format(test_loss, test_acc))\n",
    "\n",
    "# Make DataFrames for training and validation\n",
    "t_df = \n",
    "v_df = \n",
    "\n",
    "# Add truth and prediction columns to each DataFrame \n",
    "t_df['train_truth'] = \n",
    "t_df['train_prob'] = 0\n",
    "v_df['val_truth'] = \n",
    "v_df['val_prob'] = 0\n",
    "\n",
    "# Now we evaluate the model on the train and validation data by calling the predict function\n",
    "train_pred_proba = \n",
    "val_pred_proba = \n",
    "\n",
    "# Add the predictions to DataFrames\n",
    "t_df['train_prob'] = train_pred_proba\n",
    "v_df['val_prob'] = val_pred_proba\n",
    "\n",
    "\n",
    "# Okay so now we have the two dataframes ready.\n",
    "# t_df has two columns for training data  (train_truth and train_prob)\n",
    "# v_df has two columns for testing data  (train_truth and train_prob)\n",
    "\n",
    "\n",
    "# Now we plot the NN output\n",
    "mybins = np.arange(0, 1.05, 0.05)\n",
    "\n",
    "# Add the values to be made a histogram out of\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.hist('''val_truth = 1 ''', bins=mybins, histtype='step',\n",
    "         label=\"Test Signal\", linewidth=3, color='xkcd:green', density=False, log=False)\n",
    "plt.hist('''val_truth = 0''', bins=mybins, histtype='step',\n",
    "         label=\"Test Background\", linewidth=3, color='xkcd:denim', density=False, log=False)\n",
    "plt.hist('''train_truth = 1''', bins=mybins, histtype='step',\n",
    "         label=\"Train Signal\", linewidth=3, color='xkcd:greenish', density=False, log=False)\n",
    "plt.hist('''train_truth = 0''', bins=mybins, histtype='step',\n",
    "         label=\"Train Background\", linewidth=3, color='xkcd:sky blue', density=False, log=False)\n",
    "plt.legend(loc='upper center')\n",
    "plt.xlabel('Score', fontsize=15)\n",
    "plt.ylabel('Examples', fontsize=15)\n",
    "plt.title(f'NN Output', fontsize=20)\n",
    "plt.xticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0], fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.show()\n",
    "plt.savefig(pp, format='pdf')\n",
    "\n",
    "\n",
    "########\n",
    "\n",
    "# Now we get the ROC curve, first for validation data\n",
    "fpr, tpr, _ = \n",
    "auc_score = \n",
    "# Now the ROC curve for training data\n",
    "fpr1, tpr1, _ = \n",
    "auc_score1 = \n",
    "\n",
    "# We plot the ROC curves together to assess the performance\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr, tpr, color='xkcd:denim blue',\n",
    "         label='Testing ROC (AUC = %0.4f)' % auc_score)\n",
    "plt.plot(fpr1, tpr1, color='xkcd:sky blue',\n",
    "         label='Training ROC (AUC = %0.4f)' % auc_score1)\n",
    "plt.legend(loc='lower right')\n",
    "plt.title(f'ROC Curve', fontsize=20)\n",
    "plt.xlabel('False Positive Rate', fontsize=20)\n",
    "plt.ylabel('True Positive Rate', fontsize=20)\n",
    "plt.xlim(0., 1.)\n",
    "plt.ylim(0., 1.)\n",
    "plt.show()\n",
    "plt.savefig(pp, format='pdf')\n",
    "\n",
    "pp.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Job!\n",
    "You have successfully completed the second assignment in this course. \n",
    "\n",
    "Given below is one of the few other ML techinques (**Optional**) you can try out - regression to find the exact distance or a multi-classifier with 3 classes where the distances are split as [500,566) [566,633) [633,700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
